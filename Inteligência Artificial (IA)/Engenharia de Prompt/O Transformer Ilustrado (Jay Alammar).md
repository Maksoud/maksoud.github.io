### [SumÃ¡rio](<https://maksoud.github.io/SumÃ¡rio>)

# ğŸ§  **Resumo: O Transformer Ilustrado (Jay Alammar)**

ğŸ”— [Artigo original](https://jalammar.github.io/illustrated-transformer/)

### ğŸ”¹ **O que Ã© o Transformer?**

O **Transformer** Ã© uma arquitetura de rede neural introduzida no artigo _â€œAttention is All You Needâ€_ (2017), que revolucionou o campo do Processamento de Linguagem Natural (NLP). Diferente de modelos anteriores, como RNNs e LSTMs, ele nÃ£o depende de processamento sequencial â€“ permitindo **paralelismo, maior velocidade e melhor desempenho em tarefas linguÃ­sticas.**

### ğŸ”¹ **Principais Componentes do Transformer**

#### 1. **Embedding**

As palavras sÃ£o convertidas em vetores (nÃºmeros) para que possam ser entendidas pelo modelo.  
â¡ Exemplo: â€œgatoâ€ â†’ `[0.12, -0.34, ...]`

#### 2. **Positional Encoding**

Como o Transformer processa tudo ao mesmo tempo (nÃ£o em sequÃªncia), ele precisa saber **a posiÃ§Ã£o de cada palavra na frase**. Isso Ã© feito com _codificaÃ§Ãµes posicionais_ que sÃ£o somadas ao embedding.

#### 3. **AtenÃ§Ã£o (Attention)**

O coraÃ§Ã£o do Transformer. A ideia Ã©:  
**"Para cada palavra, qual Ã© a importÃ¢ncia das outras palavras no contexto?"**

A fÃ³rmula de atenÃ§Ã£o bÃ¡sica:

```
Attention(Q, K, V) = softmax(Q Ã— Káµ— / âˆšdâ‚–) Ã— V
```

Onde:

- **Q (Query)**: O que estamos procurando
- **K (Key)**: As palavras disponÃ­veis
- **V (Value)**: A informaÃ§Ã£o a ser usada

Essa tÃ©cnica permite que o modelo **"preste atenÃ§Ã£o" em partes relevantes da frase** ao tomar decisÃµes.

#### 4. **Multi-Head Attention**

O modelo usa mÃºltiplas "cabeÃ§as de atenÃ§Ã£o" que analisam diferentes partes da frase de maneira paralela e combinam os resultados.

#### 5. **Feed Forward Layer**

ApÃ³s a atenÃ§Ã£o, os dados passam por uma rede neural tradicional para mais processamento.

#### 6. **Camadas e NormalizaÃ§Ã£o**

Cada etapa tem uma **normalizaÃ§Ã£o** (layer norm) e uma **conexÃ£o residual** (skip connection), ajudando no aprendizado e estabilidade.

### ğŸ”¹ **Arquitetura Geral**

O Transformer completo Ã© formado por dois blocos principais:

- **Codificador (Encoder)**: LÃª o texto de entrada
- **Decodificador (Decoder)**: Gera o texto de saÃ­da

No caso do GPT, **apenas o decodificador Ã© usado**, pois ele Ã© treinado para prever a prÃ³xima palavra.

### ğŸ”¹ **AplicaÃ§Ãµes**

O Transformer Ã© a base de modelos como:

- **BERT** (usa sÃ³ o encoder)
- **GPT-2 / GPT-3 / GPT-4** (usam sÃ³ o decoder)
- **T5, BART** (usam encoder + decoder)


### ğŸ§  **Conceitos Fundamentais Aprendidos**

- A arquitetura Transformer Ã© baseada em atenÃ§Ã£o, nÃ£o em sequÃªncia
- O modelo aprende a focar em palavras relevantes no contexto
- Permite paralelismo, o que acelera o treinamento
- Ã‰ altamente escalÃ¡vel e adaptÃ¡vel a vÃ¡rias tarefas de linguagem
