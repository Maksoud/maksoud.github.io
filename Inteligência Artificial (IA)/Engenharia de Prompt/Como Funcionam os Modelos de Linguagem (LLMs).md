## **Como Funcionam os Modelos de Linguagem (LLMs)**

### ðŸŽ¯ **Objetivo de Aprendizado**

Compreender os conceitos fundamentais por trÃ¡s de modelos como o GPT, como eles aprendem, operam e geram texto. Isso inclui:

- Tokens e embeddings
- Arquitetura Transformer
- Ajuste fino (fine-tuning) vs. engenharia de prompt
- LimitaÃ§Ãµes e vantagens

## ðŸ§  **O Que SÃ£o LLMs?**

Modelos de Linguagem de Grande Escala (LLMs) como GPT-4, Claude e Gemini sÃ£o algoritmos baseados em **redes neurais profundas** treinados com grandes volumes de texto. Eles **predizem a prÃ³xima palavra** em uma sequÃªncia, entendendo padrÃµes e contextos.

## ðŸ“Œ **ConteÃºdo de Estudo Recomendado**

### 1. **Artigos TÃ©cnicos (leitura fundamental)**

- [ðŸ”— The Illustrated Transformer (Jay Alammar)](https://jalammar.github.io/illustrated-transformer/) â€“ Visual e intuitivo ([Resumo](<obsidian://open?vault=Obsidian%20Vault&file=maksoud.github.io%2FIntelig%C3%AAncia%20Artificial%20(IA)%2FEngenharia%20de%20Prompt%2FO%20Transformer%20Ilustrado%20(Jay%20Alammar)>))
- [ðŸ”— Como funciona o ChatGPT â€“ OpenAI](<obsidian://open?vault=Obsidian%20Vault&file=maksoud.github.io%2FIntelig%C3%AAncia%20Artificial%20(IA)%2FEngenharia%20de%20Prompt%2FComo%20Funciona%20o%20ChatGPT%20%E2%80%93%20OpenAI>)
- [ðŸ”— Understanding LLMs â€“ Sebastian Raschka (GitHub)](https://sebastianraschka.com/blog/2023/llm-overview.html)

### 2. **VÃ­deos Explicativos**

- [ðŸ“º How ChatGPT Actually Works â€“ Fireship (YouTube)](https://www.youtube.com/watch?v=JTxsNm9IdYU)  
    _ExplicaÃ§Ã£o simples e direta em 5 minutos._
- [ðŸ“º Transformers - State of the Art NLP](https://www.youtube.com/watch?v=U0s0f995w14) â€“ Yannic Kilcher

### 3. **Conceitos-chave a entender**

|Conceito|DescriÃ§Ã£o curta|
|---|---|
|**Token**|Fragmento de texto (palavra, sÃ­laba ou letra) que o modelo processa|
|**Embedding**|RepresentaÃ§Ã£o vetorial de palavras em um espaÃ§o semÃ¢ntico|
|**Transformer**|Arquitetura usada nos LLMs baseada em _atenÃ§Ã£o_ para entender contexto|
|**Self-Attention**|TÃ©cnica que permite o modelo dar "peso" diferente para palavras no contexto|
|**Pretraining**|Fase em que o modelo aprende com bilhÃµes de textos|
|**Fine-tuning**|Ajustes finais com dados especÃ­ficos (ex: ChatGPT para diÃ¡logo)|
|**Inference**|Processo de gerar texto com base em um prompt|

### 4. **ExercÃ­cio prÃ¡tico sugerido**

**Tarefa:** Responder com suas palavras:

> â€œComo um modelo como o GPT-4 gera uma resposta a partir de um prompt?â€  
> **Dica:** Use as palavras: _tokens_, _contexto_, _atenÃ§Ã£o_, _probabilidade_ e _geraÃ§Ã£o de texto_.

### 5. **Ferramentas Ãºteis**

- [OpenAI Tokenizer](https://platform.openai.com/tokenizer) â€“ Para ver como o modelo divide textos
- [Google Colab â€“ Transformer em Python](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb) â€“ Para quem quiser ir alÃ©m

## âœ… **Checklist de ConclusÃ£o**

-  Li e compreendi o funcionamento de tokens e embeddings
-  Sei explicar como funciona a arquitetura Transformer
-  Sei a diferenÃ§a entre pretraining, fine-tuning e prompting
-  Fiz um resumo pessoal ou artigo explicativo
-  Testei o Tokenizer da OpenAI com textos diferentes
